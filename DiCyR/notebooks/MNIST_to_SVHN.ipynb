{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from torch import nn\n",
    "from data_utils import load_mnist, load_svhn\n",
    "from models import DisentangledDomainAdaptationNetwork, get_simple_classifier\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from utils import show_decoded_images, plot_results\n",
    "from train import train_domain_adaptation\n",
    "from utils import test_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "target_train_loader, target_test_loader = load_svhn(img_size=(32, 32), batch_size=64, split=1, shuffle=True, num_workers=2)\n",
    "source_train_loader, source_test_loader = load_mnist(img_size=32, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, conv_feat_size, nb_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.conv_feat_size = conv_feat_size\n",
    "\n",
    "        self.deco_dense = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_space_dim, out_features=1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=1024, out_features=np.prod(self.conv_feat_size)),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.deco_fetures = nn.Sequential(\n",
    "            nn.Conv2d(self.conv_feat_size[0], out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=nb_channels, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_share, z_spe):\n",
    "        z = torch.cat([z_share, z_spe], 1)\n",
    "        feat_encode = self.deco_dense(z)\n",
    "        feat_encode = feat_encode.view(-1, *self.conv_feat_size)\n",
    "        y = self.deco_fetures(feat_encode)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, img_size, nb_channels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.nb_channels = nb_channels\n",
    "\n",
    "        self.conv_feat = nn.Sequential(\n",
    "            nn.Conv2d(nb_channels, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.conv_feat_size = self.conv_feat(torch.zeros(1, *img_size)).shape[1:]\n",
    "        self.dense_feature_size = np.prod(self.conv_feat_size)\n",
    "\n",
    "        self.dense_feat = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dense_feature_size, out_features=1024),\n",
    "            nn.ReLU(True), )\n",
    "\n",
    "        self.share_feat = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=latent_space_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.source_feat = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=latent_space_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.target_feat = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=latent_space_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = self.dense_feat(feat)\n",
    "        z_share = self.share_feat(feat)\n",
    "        z_source = self.source_feat(feat)\n",
    "        z_target = self.target_feat(feat)\n",
    "        return z_share, z_source, z_target\n",
    "\n",
    "    def forward_share(self, input_data):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = self.dense_feat(feat)\n",
    "        z_share = self.share_feat(feat)\n",
    "        return z_share\n",
    "\n",
    "    def forward_source(self, input_data):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = self.dense_feat(feat)\n",
    "        z_source = self.source_feat(feat)\n",
    "        return z_source\n",
    "\n",
    "    def forward_target(self, input_data):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = self.dense_feat(feat)\n",
    "        z_target = self.target_feat(feat)\n",
    "        return z_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_classifier(latent_space_dim=1024):\n",
    "    return nn.Sequential(nn.Dropout2d(0.55),\n",
    "                         nn.Linear(in_features=latent_space_dim, out_features=10),\n",
    "                         nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"Extension of grad reverse layer.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.neg()\n",
    "        return grad_output, None\n",
    "\n",
    "    def grad_reverse(x):\n",
    "        return GradReverse.apply(x)\n",
    "\n",
    "\n",
    "class DisentangledDomainAdaptationNetwork(nn.Module):\n",
    "    def __init__(self, encoder, decoder_source, decoder_target, classifier):\n",
    "        super(DisentangledDomainAdaptationNetwork, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder_source = decoder_source\n",
    "        self.decoder_target = decoder_target\n",
    "        self.classifier = classifier\n",
    "        latent_space_dim = self.encoder.latent_space_dim\n",
    "\n",
    "        self.random_source = nn.Sequential(\n",
    "            nn.Linear(latent_space_dim, 50))\n",
    "\n",
    "        self.random_target = nn.Sequential(\n",
    "            nn.Linear(latent_space_dim, 50))\n",
    "\n",
    "        self.random_share = nn.Sequential(\n",
    "            nn.Linear(latent_space_dim, 50))\n",
    "\n",
    "        self.spe_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_space_dim, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 50))\n",
    "\n",
    "        self.share_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_space_dim, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 50))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_share = self.encoder.forward_share(x)\n",
    "        logits = self.classifier(z_share)\n",
    "        return logits\n",
    "\n",
    "    def forward_s(self, x):\n",
    "        z_share, z_source, _ = self.encoder(x)\n",
    "        reco_s = self.decoder_source(z_share, z_source)\n",
    "        logits = self.classifier(z_share)\n",
    "\n",
    "        z_spe_rev = GradReverse.grad_reverse(z_source)\n",
    "        z_share_rev = GradReverse.grad_reverse(z_share)\n",
    "        pred_spe = torch.cos(self.spe_predictor(z_share_rev))\n",
    "        pred_share = torch.cos(self.share_predictor(z_spe_rev))\n",
    "        random_spe = torch.cos(self.random_source(z_source))\n",
    "        random_share = torch.cos(self.random_share(z_share))\n",
    "\n",
    "        return reco_s, logits, (z_share, z_source), (random_share, random_spe), (pred_share, pred_spe)\n",
    "\n",
    "    def forward_s_rand(self, z_share, x_rand):\n",
    "        z_source = self.encoder.forward_source(x_rand)\n",
    "        reco_s_rand = self.decoder_source(z_share, z_source)\n",
    "        return reco_s_rand\n",
    "\n",
    "    def forward_t(self, x):\n",
    "        z_share, _, z_target = self.encoder(x)\n",
    "        reco_t = self.decoder_target(z_share, z_target)\n",
    "        logits = self.classifier(z_share)\n",
    "\n",
    "        z_spe_rev = GradReverse.grad_reverse(z_target)\n",
    "        z_share_rev = GradReverse.grad_reverse(z_share)\n",
    "        pred_spe = torch.cos(self.spe_predictor(z_share_rev))\n",
    "        pred_share = torch.cos(self.share_predictor(z_spe_rev))\n",
    "        random_spe = torch.cos(self.random_target(z_target))\n",
    "        random_share = torch.cos(self.random_share(z_share))\n",
    "\n",
    "        return reco_t, logits, (z_share, z_target), (random_share, random_spe), (pred_share, pred_spe)\n",
    "\n",
    "    def forward_t_rand(self, z_share, x_rand):\n",
    "        z_target = self.encoder.forward_target(x_rand)\n",
    "        reco_t_rand = self.decoder_target(z_share, z_target)\n",
    "        return reco_t_rand\n",
    "\n",
    "    def forward_st(self, z_source, x):\n",
    "        z_share = self.encoder.forward_share(x)\n",
    "        reco_st = self.decoder_source(z_share, z_source)\n",
    "        logits = self.classifier(z_share)\n",
    "        return reco_st, logits, z_share\n",
    "\n",
    "    def forward_ts(self, z_target, x):\n",
    "        z_share = self.encoder.forward_share(x)\n",
    "        reco_ts = self.decoder_target(z_share, z_target)\n",
    "        logits = self.classifier(z_share)\n",
    "        return reco_ts, logits, z_share\n",
    "\n",
    "    def forward_target(self, x):\n",
    "        z_share, _, z_target = self.encoder(x)\n",
    "        logits = self.classifier(z_share)\n",
    "        return logits, z_target\n",
    "\n",
    "    def forward_source(self, x):\n",
    "        z_share, z_source, _ = self.encoder(x)\n",
    "        logits = self.classifier(z_share)\n",
    "\n",
    "        return logits, z_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_domain_adaptation(model, optimizer, source_train_loader, target_train_loader,\n",
    "                                                epochs=30, beta_max=10, running_beta=20, alpha=1, show_images=False):\n",
    "    criterion_reconstruction = nn.BCELoss()\n",
    "    criterion_classifier = nn.NLLLoss(reduction='mean')\n",
    "    criterion_weighted_classifier = nn.NLLLoss(reduction='none')\n",
    "    criterion_disentangle = nn.MSELoss()\n",
    "    criterion_distance = nn.MSELoss()\n",
    "    criterion_triplet = nn.TripletMarginLoss(margin=1)\n",
    "\n",
    "    betas = np.zeros(epochs)\n",
    "    betas[:running_beta] = np.linspace(0, beta_max, running_beta)\n",
    "    betas[running_beta:] = np.ones(epochs - running_beta) * beta_max\n",
    "\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "        total_loss = 0\n",
    "        corrects_source = 0\n",
    "        corrects_target = 0\n",
    "        total = 0\n",
    "\n",
    "        # random images used for disentanglement\n",
    "        xs_rand = next(iter(source_train_loader))[0].cuda()\n",
    "        xt_rand = next(iter(target_train_loader))[0].cuda()\n",
    "\n",
    "        for (x_s, y_s), (x_t, y_t) in zip(source_train_loader, target_train_loader):\n",
    "            loss = 0\n",
    "            x_s, y_s, x_t, y_t = x_s.cuda(), y_s.cuda(), x_t.cuda(), y_t.cuda()\n",
    "\n",
    "            # target batch\n",
    "            xt_hat, yt_hat, (z_share, z_target), (random_share, random_spe), (pred_share, pred_spe) = model.forward_t(x_t)\n",
    "            xts = model.forward_s_rand(z_share, xs_rand[:len(x_t)])\n",
    "            z_s = model.encoder.forward_share(xts.detach())\n",
    "            z_target_prime = model.encoder.forward_target(xt_rand[:len(x_t)])\n",
    "            xt_prime = model.decoder_target(z_share, z_target_prime)\n",
    "            yt_tilde, z_target_tilde = model.forward_target(xt_prime)\n",
    "\n",
    "            w, predicted = yt_hat.max(1)\n",
    "            corrects_target += predicted.eq(y_t).sum().item()\n",
    "\n",
    "            loss += alpha * criterion_reconstruction(xt_hat, x_t)\n",
    "            loss += betas[epoch] * criterion_distance(z_share, z_s)\n",
    "            loss += criterion_disentangle(pred_share, random_share) + criterion_disentangle(pred_spe, random_spe)\n",
    "            loss += 10 * torch.mean((torch.exp(w.detach()) * criterion_weighted_classifier(yt_tilde, predicted.detach())))\n",
    "            loss += 10 * criterion_triplet(z_target_tilde, z_target_prime, z_target)\n",
    "\n",
    "            # source batch\n",
    "            xs_hat, ys_hat, (z_share, z_source), (random_share, random_spe), (pred_share, pred_spe) = model.forward_s(x_s)\n",
    "            xst = model.forward_t_rand(z_share, xt_rand[:len(x_s)])\n",
    "            z_t = model.encoder.forward_share(xst.detach())\n",
    "            z_source_prime = model.encoder.forward_source(xs_rand[:len(x_s)])\n",
    "            xs_prime = model.decoder_source(z_share, z_source_prime)\n",
    "            ys_tilde, z_source_tilde = model.forward_source(xs_prime)\n",
    "\n",
    "            _, predicted = ys_hat.max(1)\n",
    "            corrects_source += predicted.eq(y_s).sum().item()\n",
    "            total += y_s.size(0)\n",
    "\n",
    "            loss += alpha * criterion_reconstruction(xs_hat, x_s)\n",
    "            loss += criterion_classifier(ys_hat, y_s)\n",
    "            loss += betas[epoch] * criterion_disentangle(z_share, z_t)\n",
    "            loss += (criterion_disentangle(pred_share, random_share) + criterion_disentangle(pred_spe, random_spe))\n",
    "            loss += 10 * criterion_classifier(ys_tilde, y_s.cuda())\n",
    "            loss += 10 * criterion_triplet(z_source_tilde, z_source_prime, z_source)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.data)\n",
    "            t.set_description(f'epoch:{epoch} current target accuracy:{round(corrects_target / total * 100, 2)}%')\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, epochs, total_loss / len(source_train_loader)))\n",
    "        print(f'accuracy source: {round(corrects_source / total * 100, 2)}%')\n",
    "        print(f'accuracy target: {round(corrects_target / total * 100, 2)}%')\n",
    "        if show_images:\n",
    "            show_decoded_images(x_s, xs_hat, xs_prime, xst)\n",
    "            show_decoded_images(x_t, xt_hat, xt_prime, xts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0 current target accuracy:16.85%:   3%|▎         | 1/30 [01:14<35:49, 74.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/30], loss:11.3741\n",
      "accuracy source: 80.29%\n",
      "accuracy target: 16.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1 current target accuracy:19.76%:   7%|▋         | 2/30 [02:28<34:38, 74.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/30], loss:4.5395\n",
      "accuracy source: 97.38%\n",
      "accuracy target: 19.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:2 current target accuracy:20.76%:  10%|█         | 3/30 [03:44<33:34, 74.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [3/30], loss:3.9021\n",
      "accuracy source: 98.2%\n",
      "accuracy target: 20.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:3 current target accuracy:19.44%:  13%|█▎        | 4/30 [04:59<32:25, 74.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/30], loss:3.5301\n",
      "accuracy source: 98.4%\n",
      "accuracy target: 19.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:4 current target accuracy:19.58%:  13%|█▎        | 4/30 [05:53<38:15, 88.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-db32cb0b859e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m train_domain_adaptation(model, optimizer, source_train_loader, target_train_loader,\n\u001b[0;32m---> 19\u001b[0;31m                                             epochs=30, beta_max=1, running_beta=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-acbab490c012>\u001b[0m in \u001b[0;36mtrain_domain_adaptation\u001b[0;34m(model, optimizer, source_train_loader, target_train_loader, epochs, beta_max, running_beta, alpha, show_images)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch:{epoch} current target accuracy:{round(corrects_target / total * 100, 2)}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# ===================log========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "\n",
    "encoder = Encoder(latent_space_dim=200, img_size=(3,32,32), nb_channels=3)\n",
    "conv_feat_size = encoder.conv_feat_size\n",
    "decoder_source = Decoder(latent_space_dim=400, conv_feat_size=conv_feat_size, nb_channels=1)\n",
    "decoder_target = Decoder(latent_space_dim=400, conv_feat_size=conv_feat_size, nb_channels=3)\n",
    "classifier = get_simple_classifier(latent_space_dim=200)\n",
    "model = DisentangledDomainAdaptationNetwork(encoder, decoder_source, decoder_target, classifier).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.encoder.parameters()},\n",
    "    {'params': model.decoder_source.parameters()},\n",
    "    {'params': model.decoder_target.parameters()},\n",
    "    {'params': model.classifier.parameters()},\n",
    "    {'params': model.spe_predictor.parameters()},\n",
    "    {'params': model.share_predictor.parameters()}], lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "train_domain_adaptation(model, optimizer, source_train_loader, target_train_loader,\n",
    "                                            epochs=30, beta_max=1, running_beta=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "train_disentangle_domain_adaptation_network(model, optimizer, source_train_loader, target_train_loader,\n",
    "      epochs=20, beta_max=10, gamma=0.9, running_beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network(model, target_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_results2(model, source_train_loader, target_train_loader):\n",
    "    x, _ = next(iter(source_train_loader))\n",
    "    \n",
    "    reco_s, pred, (z_share, z_source), (random_share, random_spe), (pred_share, pred_spe) = model.forward_s(x.cuda())\n",
    "    _, predicted = pred.max(1)\n",
    "    \n",
    "    t = []\n",
    "    nb_ex = 5\n",
    "    for k in range(nb_ex):\n",
    "        x_rand_t = next(iter(target_train_loader))[0].cuda()\n",
    "        t.append(model.forward_t_rand(z_share, x_rand_t[:len(x)]))\n",
    "      \n",
    "\n",
    "    for i in range(32):\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.subplot(1, nb_ex + 1, 1)\n",
    "        plt.imshow(x[i].cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'prediction: {int(predicted[i])}')\n",
    "        for k in range(nb_ex):\n",
    "            plt.subplot(1, nb_ex + 1, k+2)\n",
    "            plt.imshow(t[k][i].cpu().detach()[0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "        \n",
    "    x, _ = next(iter(target_train_loader))\n",
    "    reco_t, pred, (z_share, z_target), (random_share, random_spe), (pred_share, pred_spe) = model.forward_t(x.cuda())\n",
    "    _, predicted = pred.max(1)\n",
    "    \n",
    "    t = []\n",
    "    for k in range(nb_ex):\n",
    "        x_rand = next(iter(source_train_loader))[0].cuda()\n",
    "        t.append(model.forward_s_rand(z_share, x_rand[:len(x)]))\n",
    "\n",
    "    for i in range(32):\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.subplot(1, nb_ex + 1, 1)\n",
    "        plt.imshow(x[i].cpu().detach()[0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'prediction: {int(predicted[i])}')\n",
    "        for k in range(nb_ex):\n",
    "            plt.subplot(1, nb_ex + 1, k+2)\n",
    "            plt.imshow(t[k][i].cpu().detach().permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "\n",
    "plot_results2(model, source_train_loader, target_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(target_train_loader))\n",
    "y, _, (z_share, z_spe),  _, _ = model.forward_t(X.cuda())\n",
    "#blank\n",
    "plt.subplot(1,6,1)\n",
    "plt.imshow(torch.ones((32,32,3)))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#styles\n",
    "for i in range(5):\n",
    "    plt.subplot(1,6,i+2)\n",
    "    plt.imshow(X[i].cpu().detach()[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "for j in range(10, 20):\n",
    "    plt.figure()\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(X[j][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    z_x = torch.zeros_like(z_share)\n",
    "    z_x[:] = z_share[j]\n",
    "    y2  = model.decoder_target(z_x, z_spe)\n",
    "    for i in range(5):\n",
    "        plt.subplot(1,6,i+2)\n",
    "        plt.imshow(y2[i].cpu().detach()[0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "def extract_features(train_loader, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 75))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    batch_size=128\n",
    "    i = 0\n",
    "    for x, labels_batch in train_loader:\n",
    "        features_batch = encoder(x.cuda())[0]\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch.cpu().detach().numpy()\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch.numpy()\n",
    "        i += 1\n",
    "        \n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s, s_labels = extract_features(source_train_loader, 640)\n",
    "f_t, t_labels = extract_features(target_train_loader, 640)\n",
    "\n",
    "f = np.zeros((1280, 75))\n",
    "f[:640] = f_s\n",
    "f[640:] = f_t\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(f)\n",
    "X_tsne = TSNE(2, perplexity=50).fit_transform(X_pca)\n",
    "X_tsne_x = X_tsne[:,0]\n",
    "X_tsne_y = X_tsne[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "color_map=plt.cm.rainbow(np.linspace(0,1,10))\n",
    "for i, g in enumerate(np.unique(s_labels)):\n",
    "    color = color_map[i]\n",
    "    ix = np.where(s_labels == g)\n",
    "    plt.scatter(x=X_tsne_x[ix], y=X_tsne_y[ix], color=color , marker='o', alpha=0.5, label = g)\n",
    "for i, g in enumerate(np.unique(t_labels)):\n",
    "    color = color_map[i]\n",
    "    ix = np.where(t_labels == g)\n",
    "    plt.scatter(x=X_tsne_x[640:][ix], y=X_tsne_y[640:][ix], color=color, marker='+', label = g)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorch.utils import predict_from_logits\n",
    "from advertorch_examples.utils import get_mnist_test_loader\n",
    "from advertorch_examples.utils import _imshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorch.attacks import LinfPGDAttack\n",
    "\n",
    "adversary = LinfPGDAttack(\n",
    "    model, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"), eps=0.15,\n",
    "    nb_iter=40, eps_iter=0.01, rand_init=True, clip_min=0.0, clip_max=1.0,\n",
    "    targeted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x, label = next(iter(source_train_loader))\n",
    "x = x.cuda()\n",
    "label = label.cuda()\n",
    "adv_untargeted = adversary.perturb(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones_like(label) * 3\n",
    "adversary.targeted = True\n",
    "adv_targeted = adversary.perturb(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cln = predict_from_logits(model(x))\n",
    "pred_untargeted_adv = predict_from_logits(model(adv_untargeted))\n",
    "pred_targeted_adv = predict_from_logits(model(adv_targeted))\n",
    "batch_size=8\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "for ii in range(batch_size):\n",
    "    plt.subplot(3, batch_size, ii + 1)\n",
    "    _imshow(x[ii])\n",
    "    plt.title(\"clean \\n pred: {}\".format(pred_cln[ii]))\n",
    "    plt.subplot(3, batch_size, ii + 1 + batch_size)\n",
    "    _imshow(adv_untargeted[ii])\n",
    "    plt.title(\"untargeted \\n adv \\n pred: {}\".format(\n",
    "        pred_untargeted_adv[ii]))\n",
    "    plt.subplot(3, batch_size, ii + 1 + batch_size * 2)\n",
    "    _imshow(adv_targeted[ii])\n",
    "    plt.title(\"targeted to 3 \\n adv \\n pred: {}\".format(\n",
    "        pred_targeted_adv[ii]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
