{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from torch import nn\n",
    "from data_utils import load_mnist, load_svhn\n",
    "from models import DomainAdaptationNetwork, get_simple_classifier, ProjectorNetwork\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from train import train_domain_adaptation\n",
    "from utils import test_network, plot_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, conv_feat_size, nb_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.conv_feat_size = conv_feat_size\n",
    "\n",
    "        self.deco_dense = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_space_dim, out_features=1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=1024, out_features=np.prod(self.conv_feat_size)),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.deco_fetures = nn.Sequential(\n",
    "            nn.Conv2d(self.conv_feat_size[0], out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nb_channels, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z_share, z_spe):\n",
    "        z = torch.cat([z_share, z_spe], 1)\n",
    "        feat_encode = self.deco_dense(z)\n",
    "        feat_encode = feat_encode.view(-1, *self.conv_feat_size)\n",
    "        y = self.deco_fetures(feat_encode)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim, img_size, nb_channels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.nb_channels = nb_channels\n",
    "\n",
    "        self.conv_feat = nn.Sequential(\n",
    "            nn.Conv2d(nb_channels, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "        )\n",
    "\n",
    "        self.conv_feat_size = self.conv_feat(torch.zeros(1, *img_size)).shape[1:]\n",
    "        self.dense_feature_size = np.prod(self.conv_feat_size)\n",
    "\n",
    "        self.dense_feat = nn.Linear(in_features=self.dense_feature_size, out_features=2048)\n",
    "        self.task_feat = nn.Linear(in_features=2048, out_features=latent_space_dim)\n",
    "        self.source_feat = nn.Linear(in_features=2048, out_features=latent_space_dim)\n",
    "        self.target_feat = nn.Linear(in_features=2048, out_features=latent_space_dim)\n",
    "\n",
    "    def forward(self, input_data, mode='all'):\n",
    "        if (input_data.shape[1] == 1) & (self.nb_channels == 3):\n",
    "            input_data = input_data.repeat(1, 3, 1, 1)\n",
    "        feat = self.conv_feat(input_data)\n",
    "        feat = feat.view(-1, self.dense_feature_size)\n",
    "        feat = F.relu(self.dense_feat(feat))\n",
    "        if mode == 'task':\n",
    "            z_task = F.relu(self.task_feat(feat))\n",
    "            return z_task\n",
    "        \n",
    "        elif mode == 'source':\n",
    "            z_source = F.relu(self.source_feat(feat))\n",
    "            return z_source\n",
    "        \n",
    "        elif mode == 'target':\n",
    "            z_target = F.relu(self.target_feat(feat))\n",
    "            return z_target\n",
    "        \n",
    "        else:\n",
    "            z_task = F.relu(self.task_feat(feat))\n",
    "            z_source = F.relu(self.source_feat(feat))\n",
    "            z_target = F.relu(self.target_feat(feat))\n",
    "            return z_task, z_source, z_target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from utils import show_decoded_images\n",
    "\n",
    "\n",
    "criterion_reconstruction = nn.BCELoss()\n",
    "criterion_classifier = nn.NLLLoss(reduction='mean')\n",
    "criterion_weighted_classifier = nn.NLLLoss(reduction='none')\n",
    "criterion_disentangle = nn.MSELoss()\n",
    "criterion_distance = nn.MSELoss()\n",
    "criterion_triplet = nn.TripletMarginLoss(margin=1)\n",
    "\n",
    "criterion_reconstruction = nn.BCELoss()\n",
    "disentangle_criterion = nn.MSELoss()\n",
    "criterion_classifier = nn.NLLLoss(reduction='mean')\n",
    "criterion_triplet = nn.TripletMarginLoss(margin=1)\n",
    "\n",
    "def train_domain_adaptation(model, optimizer, random_projector, source_train_loader, target_train_loader, betas,\n",
    "                            alpha=1, gamma=1, delta=0.1, epochs=30, show_images=False):\n",
    "\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "        total_loss = 0\n",
    "        corrects_source, corrects_target = 0, 0\n",
    "        total_source, total_target = 0, 0\n",
    "\n",
    "        # random images used for disentanglement\n",
    "        #xs_rand = next(iter(source_train_loader))[0].cuda()\n",
    "        #xt_rand = next(iter(target_train_loader))[0].cuda()\n",
    "\n",
    "        for (x_s, y_s), (x_t, y_t) in zip(source_train_loader, target_train_loader):\n",
    "            loss = 0\n",
    "            x_s, y_s, x_t, y_t = x_s.cuda(), y_s.cuda(), x_t.cuda(), y_t.cuda()\n",
    "            min_len = min(len(x_s), len(x_t))\n",
    "            x_s, y_s, x_t, y_t = x_s[:min_len], y_s[:min_len], x_t[:min_len], y_t[:min_len]\n",
    "            # target batch\n",
    "            xt_hat, yt_hat, (z_task, z_target), (pred_task, pred_spe) = model(x_t, mode='all_target')\n",
    "            #Random projection to reduce the dimension\n",
    "            random_task = random_projector(z_task)\n",
    "            random_spe = random_projector(z_target)\n",
    "            \n",
    "            # synthetic sample with task information from x_t and style info from xs_rand\n",
    "            xts = model.decode(z_task, x_s[:len(x_t)], mode='source')\n",
    "            z_s = model.encoder(xts.detach(), mode='task')\n",
    "            z_target_prime = model.encoder(torch.flip(x_t, [0])[:len(x_t)], mode='target')\n",
    "            xt_prime = model.decoder_target(z_task, z_target_prime)\n",
    "            yt_tilde, z_target_tilde = model.forward(xt_prime, mode='target')\n",
    "\n",
    "            w, predicted = yt_hat.max(1)\n",
    "            corrects_target += predicted.eq(y_t).sum().item()\n",
    "            total_target += y_t.size(0)\n",
    "\n",
    "            loss += alpha * criterion_reconstruction(xt_hat, x_t)\n",
    "            loss += betas[epoch] * criterion_distance(z_task, z_s)\n",
    "            loss += gamma * (criterion_disentangle(pred_task, random_task) + criterion_disentangle(pred_spe, random_spe))\n",
    "            loss += 0.1 * torch.mean((torch.exp(w.detach()) * criterion_weighted_classifier(yt_tilde, predicted.detach())))\n",
    "            loss += delta[epoch] * criterion_triplet(z_target_tilde, z_target_prime, z_target)\n",
    "\n",
    "            # source batch\n",
    "            xs_hat, ys_hat, (z_task, z_source), (pred_task, pred_spe) = model(x_s, mode='all_source')\n",
    "            #Random projection to reduce the dimension\n",
    "            random_task = random_projector(z_task)\n",
    "            random_spe = random_projector(z_source)\n",
    "            \n",
    "            # synthetic sample with task information from x_s and style info from xt_rand\n",
    "            xst = model.decode(z_task, x_t[:len(x_s)], mode='target')\n",
    "            z_t = model.encoder(xst.detach(), mode='task')\n",
    "            z_source_prime = model.encoder(torch.flip(x_s, [0])[:len(x_s)], mode='source')\n",
    "            xs_prime = model.decoder_source(z_task, z_source_prime)\n",
    "            ys_tilde, z_source_tilde = model(xs_prime, mode='source')\n",
    "\n",
    "            _, predicted = ys_hat.max(1)\n",
    "            corrects_source += predicted.eq(y_s).sum().item()\n",
    "            total_source += y_s.size(0)\n",
    "\n",
    "            loss += criterion_classifier(ys_hat, y_s)\n",
    "            loss += alpha * criterion_reconstruction(xs_hat, x_s)\n",
    "            loss += betas[epoch] * criterion_distance(z_task, z_t)\n",
    "            loss += gamma * (criterion_disentangle(pred_task, random_task) + criterion_disentangle(pred_spe, random_spe))\n",
    "            loss += 0.1 * criterion_classifier(ys_tilde, y_s.cuda())\n",
    "            loss += delta[epoch] * criterion_triplet(z_source_tilde, z_source_prime, z_source)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.data)\n",
    "            t.set_description(f'epoch:{epoch} current target accuracy:{round(corrects_target / total_source * 100, 2)}%')\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, epochs, total_loss / len(source_train_loader)))\n",
    "        print(f'accuracy source: {round(corrects_source / total_source * 100, 2)}%')\n",
    "        print(f'accuracy target: {round(corrects_target / total_target * 100, 2)}%')\n",
    "        if show_images:\n",
    "            show_decoded_images(x_s[:16], xs_hat[:16], x_t[:len(x_s)][:16], xst[:16])\n",
    "            show_decoded_images(x_t[:16], xt_hat[:16], x_s[:len(x_t)][:16], xts[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0 current target accuracy:41.84%:   3%|▎         | 1/30 [00:38<18:46, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/30], loss:1.6631\n",
      "accuracy source: 68.11%\n",
      "accuracy target: 41.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1 current target accuracy:74.88%:   7%|▋         | 2/30 [01:17<18:08, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/30], loss:1.3746\n",
      "accuracy source: 86.21%\n",
      "accuracy target: 74.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:2 current target accuracy:82.69%:  10%|█         | 3/30 [01:56<17:29, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [3/30], loss:1.1834\n",
      "accuracy source: 89.7%\n",
      "accuracy target: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:3 current target accuracy:84.32%:  13%|█▎        | 4/30 [02:35<16:50, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/30], loss:1.1197\n",
      "accuracy source: 91.16%\n",
      "accuracy target: 84.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:4 current target accuracy:87.29%:  13%|█▎        | 4/30 [02:51<18:34, 42.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2b0613843ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m train_domain_adaptation(model, optimizer, random_projector, source_train_loader, target_train_loader, betas=betas,\n\u001b[0;32m---> 21\u001b[0;31m                                             epochs=epochs, alpha=1, delta=delta, gamma=1, show_images=False)\n\u001b[0m",
      "\u001b[0;32m~/Workspace/Disentangled_domain_adaptation_code/DiCyR/train.py\u001b[0m in \u001b[0;36mtrain_domain_adaptation\u001b[0;34m(model, optimizer, random_projector, source_train_loader, target_train_loader, betas, alpha, gamma, delta, epochs, show_images)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch:{epoch} current target accuracy:{round(corrects_target / total_source * 100, 2)}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# ===================log========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_train_loader, target_test_loader = load_mnist(img_size=32, batch_size=128, shuffle=True, num_workers=4)\n",
    "source_train_loader, source_test_loader = load_svhn(img_size=(32, 32), batch_size=128, split=1, shuffle=True, num_workers=4)\n",
    "\n",
    "learning_rate = 5e-4\n",
    "#epochs=10\n",
    "epochs=30\n",
    "\n",
    "encoder = Encoder(latent_space_dim=75, img_size=(3,32,32), nb_channels=3)\n",
    "conv_feat_size = encoder.conv_feat_size\n",
    "decoder_source = Decoder(latent_space_dim=150, conv_feat_size=conv_feat_size, nb_channels=3)\n",
    "decoder_target = Decoder(latent_space_dim=150, conv_feat_size=conv_feat_size, nb_channels=1)\n",
    "classifier = get_simple_classifier(latent_space_dim=75)\n",
    "model = DomainAdaptationNetwork(encoder, decoder_source, decoder_target, classifier).cuda()\n",
    "random_projector = ProjectorNetwork(latent_dim=75).cuda()\n",
    "betas = np.ones(30) * 10\n",
    "betas[0:10] = np.linspace(0, 10, 10)\n",
    "delta = np.ones(30)# * 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0.001)\n",
    "\n",
    "train_domain_adaptation(model, optimizer, random_projector, source_train_loader, target_train_loader, betas=betas,\n",
    "                                            epochs=epochs, alpha=1, delta=delta, gamma=1, show_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_network(model, target_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_target_cross_domain_swapping(model, source_train_loader, target_train_loader):\n",
    "    X, _ = next(iter(target_train_loader))\n",
    "    y, _, (z_share, z_spe),  _ = model(X.cuda(), mode='all_target')\n",
    "    X2, _ = next(iter(source_train_loader))\n",
    "    _, _, (z_share, _),  _ = model(X2.cuda(), mode='all_target')\n",
    "    #blank\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(torch.ones((32,32,3)))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    #styles\n",
    "    for i in range(5):\n",
    "        plt.subplot(1,6,i+2)\n",
    "        plt.imshow(X[i].cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    for j in range(10, 20):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,6,1)\n",
    "        plt.imshow(X2[j].cpu().detach().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        z_x = torch.zeros_like(z_share)\n",
    "        z_x[:] = z_share[j]\n",
    "        y2  = model.decoder_target(z_x, z_spe)\n",
    "        for i in range(5):\n",
    "            plt.subplot(1,6,i+2)\n",
    "            plt.imshow(y2[i].cpu().detach().permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_cross_domain_swapping(model, source_train_loader, target_train_loader)\n",
    "plot_tsne(model, source_train_loader, target_train_loader, 128, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
